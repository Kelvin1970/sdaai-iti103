{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-pdc1-students/blob/master/ml-essentials/session-6/Regularization_GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Gradient Descent\n",
    "\n",
    "Welcome to the programming exercise. This is part of the series of exercises to help you acquire skills in different techniques to fine-tune your model.\n",
    "\n",
    "**You will learn:**\n",
    "- how regularization can be used to avoid overfitting the data\n",
    "- effects of different regularization techniques (e.g. L1/L2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regularization  \n",
    "\n",
    "We will begin with a short tutorial on regularization based on a very simple generated 'noisy' dataset and examine the effects of regularization on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# common import\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import a Data Set and Plot the Data\n",
    "\n",
    "Import the file 'X_Y_Sinusoid_Data.csv' which contains a noisy set of x and y values that corresponds to a function $y = sin(2\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following two lines if running the notebook from local machine\n",
    "#data_path = ['data']\n",
    "#filepath = os.sep.join(data_path + ['X_Y_Sinusoid_data.csv'])\n",
    "\n",
    "## If running in colab, read the data from url\n",
    "filepath = 'https://raw.githubusercontent.com/nyp-sit/data/master/X_Y_Sinusoid_Data.csv'\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a set of x and y values that corresponds to the ground truth $y = sin(2\\pi x)$ and plot the sparse data (`x` vs `y`) and the calculated (\"real\") data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "Generate 100 equally spaced x data points over the range of 0 to 1. Using these points, calculate the y-data which represents the \"ground truth\" (the real function) from the equation: $y = sin(2\\pi x)$\n",
    "\n",
    "***Hint:***\n",
    "- use the np.linspace() to generate the required x values\n",
    "- use the np.sin() for the sine function and np.pi for the constant $\\pi$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "X_real = None \n",
    "Y_real = None\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the 'noisy' data\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(data['x'], data['y'], \"o\", label='data')\n",
    "plt.xlabel(\"x data\", fontsize=18)\n",
    "plt.ylabel(\"y data\", fontsize=18, rotation='1')\n",
    "#plot the real function\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Fit the model with higher-oder polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "Using the [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class from Scikit-learn's preprocessing library, create 20th order polynomial features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Extract the X- and Y- data from the dataframe \n",
    "X_data = None\n",
    "Y_data = None \n",
    "\n",
    "# Setup the polynomial features\n",
    "degree = 0\n",
    "pf = None\n",
    "\n",
    "# Create the polynomial features\n",
    "X_poly = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "#print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_data has the 20 data points. What do you think is the shape of X_poly? \n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<p>\n",
    "\n",
    "(20,21). Although we specify degree 20 for the polynomial features, 21 features were generated because of the additional bias term. You can omit the bias term by specifying:\n",
    "    \n",
    "```python\n",
    "pf = PolynomialFeatures(degree, include_bias=False)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit this data using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr = lr.fit(X_poly, Y_data)\n",
    "Y_pred = lr.predict(X_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting predicted value compared to the calculated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "# Plot the result\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, marker='^', alpha=0.5, label='pred w/ polynomial features')\n",
    "plt.legend()\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise:***\n",
    "\n",
    "What can you observe from the graph about the linear regression model trained with 20th degree polynomial features?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<p>\n",
    "The model overfits the data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Use Regularized Model\n",
    "\n",
    "Now we will try to use the regularized model such as Ridge and Lasso to fit the data with 20th degree polynomial features and observe the difference. \n",
    "\n",
    "***Exercise:***\n",
    "\n",
    "- Perform the regression on using the data with polynomial features using ridge regression ($\\alpha$=0.001) and lasso regression ($\\alpha$=0.0001). \n",
    "- Plot the results, as was done in section 1.3. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Fit with ridge regression model\n",
    "ridge = None \n",
    "Y_pred_rr = None\n",
    "\n",
    "# Similarly, fit the data with lasso regression model\n",
    "\n",
    "lassor = None \n",
    "Y_pred_lr = None\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# The plot of the predicted values\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
    "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
    "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_rr, label='ridge regression', marker='^', alpha=.5)\n",
    "plt.plot(X_data, Y_pred_lr, label='lasso regression', marker='^', alpha=.5)\n",
    "plt.xlabel('x data')\n",
    "plt.ylabel('y data')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the absolute value of coefficients for each model\n",
    "\n",
    "coefficients = pd.DataFrame()\n",
    "coefficients['linear regression'] = lr.coef_.ravel()\n",
    "coefficients['ridge regression'] = ridge.coef_.ravel()\n",
    "coefficients['lasso regression'] = lassor.coef_.ravel()\n",
    "coefficients = coefficients.applymap(abs)\n",
    "\n",
    "coefficients.describe()  # Huge difference in scale between non-regularized vs regularized regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients is plot using its own y-axis due to their much larger magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the dual y-axes\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the coefficients of unregularized linear regression model \n",
    "# Note: lr.coef_ is a 2-D tensor of shape(1,20)\n",
    "# we need to change it to 1-D array for plotting by calling .ravel()\n",
    "ax1.plot(lr.coef_.ravel(), \n",
    "         color='r', marker='o', label='linear regression')\n",
    "\n",
    "# Plot the coefficients of ridge model\n",
    "ax2.plot(ridge.coef_.ravel(), \n",
    "         color='b', marker='o', label='ridge regression')\n",
    "\n",
    "# Plot the coefficients of lassor model\n",
    "ax2.plot(lassor.coef_.ravel(), \n",
    "         color='g', marker='o', label='lasso regression')\n",
    "\n",
    "# Customize axes scales\n",
    "ax1.set_ylim(-2e14, 2e14)   # for unregularized linear regression \n",
    "ax2.set_ylim(-25, 25)       # for regularized linear regression\n",
    "\n",
    "# Combine the legends\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2)\n",
    "\n",
    "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
    "ax2.set(ylabel='ridge and lasso regression')\n",
    "\n",
    "ax1.set_xticks(range(len(lr.coef_.ravel())));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercises:***\n",
    "\n",
    "- What is the effect of regularization on the size of the coefficients?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<p>\n",
    "Regularization shrinks the size of coefficients to avoid overfitting\n",
    "</p>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "- Which (Ridge or Lasso) regularization make most of the coefficients goes to 0?\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "<p>\n",
    "Lasso\n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
